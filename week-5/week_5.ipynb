{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ac6d92",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b552858",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce380c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in the previous experiments\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# for training\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# for experiment tracking\n",
    "import wandb\n",
    "\n",
    "\n",
    "# common packages\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6306412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "login(token=os.getenv(\"HUGGINGFACE_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8d3d8",
   "metadata": {},
   "source": [
    "## Dataset and Setup Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Helper function to fetch first 500 and add language ID\n",
    "def get_subset(dataset_name, lang_id, count=500):\n",
    "    ds = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    # Use itertools.islice to take the first `count` examples\n",
    "    subset = list(itertools.islice(ds, count))\n",
    "    for example in subset:\n",
    "        example[\"lang\"] = lang_id\n",
    "    return subset\n",
    "\n",
    "# Load subsets with streaming\n",
    "tamil_subset = get_subset(\"SPRINGLab/IndicVoices-R_Tamil\", lang_id=0)\n",
    "hindi_subset = get_subset(\"SPRINGLab/IndicVoices-R_Hindi\", lang_id=1)\n",
    "bengali_subset = get_subset(\"SPRINGLab/IndicVoices-R_Bengali\", lang_id=2)\n",
    "\n",
    "# Combine all subsets\n",
    "combined = tamil_subset + hindi_subset + bengali_subset\n",
    "\n",
    "# Shuffle combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "final_dataset = Dataset.from_list(combined)\n",
    "\n",
    "# Optional: Print an example\n",
    "print(final_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f576f93",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f424e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_index = random.randint(0, 1499)\n",
    "final_dataset[ran_index]['audio']['sampling_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865f3e2",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40991aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_subset[14]['speaker_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e7ff6",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading wav2vec model to extract features from audio files.\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from tqdm import tqdm\n",
    "# Load the Wav2Vec2 processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "call_count=0\n",
    "# def extract_wav2vec_features(batch):\n",
    "#     global call_count\n",
    "#     call_count += 1\n",
    "#     print(f\"Processing batch {call_count}\")\n",
    "\n",
    "#     # Get all audio arrays in the batch\n",
    "#     audio_arrays = [sample[\"array\"] for sample in batch[\"audio\"]]\n",
    "#     sampling_rate = 16000  # adjust if your audio isn't actually at 48kHz\n",
    "\n",
    "#     # Tokenize with padding\n",
    "#     inputs = processor(audio_arrays, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "#     # Extract 9th and 11th layer hidden states and average\n",
    "#     hs_9 = outputs.hidden_states[9]\n",
    "#     hs_11 = outputs.hidden_states[11]\n",
    "#     averaged_features = ((hs_9 + hs_11) / 2).mean(dim=1)  # mean over time (seq length)\n",
    "\n",
    "#     return {\n",
    "#         \"features\": [feat.numpy() for feat in averaged_features],  # one per sample\n",
    "#         \"label\": batch[\"lang\"]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # move model to GPU\n",
    "\n",
    "def extract_wav2vec_features(batch):\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    print(f\"Processing batch {call_count}\")\n",
    "\n",
    "    # Get audio arrays\n",
    "    audio_arrays = [sample[\"array\"] for sample in batch[\"audio\"]]\n",
    "    sampling_rate = 16000\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = processor(audio_arrays, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Move input tensors to GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Get 9th and 11th layer hidden states\n",
    "    hs_9 = outputs.hidden_states[9]\n",
    "    hs_11 = outputs.hidden_states[11]\n",
    "\n",
    "    # Average over time (dimension 1)\n",
    "    averaged_features = ((hs_9 + hs_11) / 2).mean(dim=1)\n",
    "\n",
    "    return {\n",
    "        \"features\": [feat.cpu().numpy() for feat in averaged_features],  # move back to CPU before converting to NumPy\n",
    "        \"label\": batch[\"lang\"]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature extraction\n",
    "dataset = final_dataset.map(extract_wav2vec_features, batched=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9cc704",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c31711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_index = random.randint(0, 1499)\n",
    "len(dataset[ran_index]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4296d3d",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa72e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['features']\n",
    "feature_matrix = np.vstack(dataset['features'])\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678c3b7",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params/1e6:0.2f} million')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407b8da",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wav2vec_featuresM(batch):\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    print(f\"Processing batch {call_count}\")\n",
    "\n",
    "    # Get audio arrays\n",
    "    audio_arrays = [sample[\"array\"] for sample in batch[\"audio\"]]\n",
    "    sampling_rate = 16000\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = processor(audio_arrays, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Move input tensors to GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Get 9th and 11th layer hidden states\n",
    "    hs_7 = outputs.hidden_states[7]\n",
    "    hs_12 = outputs.hidden_states[12]\n",
    "\n",
    "    # Average over time (dimension 1)\n",
    "    averaged_features = ((hs_7 + hs_12) / 2).mean(dim=1)\n",
    "\n",
    "    return {\n",
    "        \"features\": [feat.cpu().numpy() for feat in averaged_features],  # move back to CPU before converting to NumPy\n",
    "        \"label\": batch[\"lang\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b9afb",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675383c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetM = final_dataset.map(extract_wav2vec_featuresM, batched=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The answer is {datasetM[0]['features'][0]:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59133832",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_subset[24]['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6eb9ea",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f70ff890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'lang', 'samples', 'verbatim', 'normalized', 'speaker_id', 'scenario', 'task_name', 'gender', 'age_group', 'job_type', 'qualification', 'area', 'district', 'state', 'occupation', 'utterance_pitch_mean', 'utterance_pitch_std', 'snr', 'c50', 'speaking_rate', 'cer', 'duration', 'audio', 'features', 'label'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef32bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetMF = dataset.select_columns(['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23e89b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['features', 'label'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "332d9dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 68.75%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract features and labels from the list of dictionaries\n",
    "X = np.array([example[\"features\"] for example in datasetMF])\n",
    "y = np.array([example[\"label\"] for example in datasetMF])\n",
    "\n",
    "# Step 1: Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Further split training set into train/validation (80/20 split)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Step 3: Train logistic regression model\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Step 4: Evaluate on validation set\n",
    "y_pred_val = clf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val) * 100  # convert to percentage\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297f14a",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f50aec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e1d54b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.00%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(y_test, y_test_pred) * 100  # convert to percentage\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5511da",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmlf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
