{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Light Image Denoising + 4x Super-Resolution (Kaggle-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:52:41.432206Z",
     "iopub.status.busy": "2025-08-23T09:52:41.431545Z",
     "iopub.status.idle": "2025-08-23T09:52:41.437391Z",
     "shell.execute_reply": "2025-08-23T09:52:41.436671Z",
     "shell.execute_reply.started": "2025-08-23T09:52:41.432181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.16\n",
      "Torch: 1.1.0\n",
      "CUDA available: False\n",
      "Platform: Linux-6.8.0-60-generic-x86_64-with-debian-trixie-sid\n",
      "WORK_DIR: /home/sachin/projects/DLP/deep-learning-practices/week-12\n",
      "WEEK12_DIR: /home/sachin/projects/DLP/deep-learning-practices\n"
     ]
    }
   ],
   "source": [
    "# Environment check and GPU\n",
    "import os, sys, platform, torch\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU count:', torch.cuda.device_count())\n",
    "    print('GPU name:', torch.cuda.get_device_name(0))\n",
    "print('Platform:', platform.platform())\n",
    "# Workspace-local data roots\n",
    "WORK_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd()))\n",
    "WEEK12_DIR = os.path.dirname(WORK_DIR)\n",
    "print('WORK_DIR:', WORK_DIR)\n",
    "print('WEEK12_DIR:', WEEK12_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:52:41.438297Z",
     "iopub.status.busy": "2025-08-23T09:52:41.438117Z",
     "iopub.status.idle": "2025-08-23T09:52:44.504697Z",
     "shell.execute_reply": "2025-08-23T09:52:44.503666Z",
     "shell.execute_reply.started": "2025-08-23T09:52:41.438282Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installs (Kaggle usually has these; safe to re-run if missing)\n",
    "%pip -q install --no-warn-script-location yacs natsort tqdm opencv-python Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities: PSNR and common helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:52:44.507038Z",
     "iopub.status.busy": "2025-08-23T09:52:44.506794Z",
     "iopub.status.idle": "2025-08-23T09:52:44.514897Z",
     "shell.execute_reply": "2025-08-23T09:52:44.514254Z",
     "shell.execute_reply.started": "2025-08-23T09:52:44.507015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, numpy as np, cv2, random\n",
    "import PIL\n",
    "# Compatibility for older torchvision that imports PILLOW_VERSION\n",
    "if not hasattr(PIL, 'PILLOW_VERSION'):\n",
    "    PIL.PILLOW_VERSION = getattr(PIL, '__version__', '0.0.0')\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(seed=1234):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def torch_psnr(tar_img, prd_img):\n",
    "    imdff = torch.clamp(prd_img,0,1) - torch.clamp(tar_img,0,1)\n",
    "    rmse = (imdff**2).mean().sqrt()\n",
    "    ps = 20*torch.log10(torch.tensor(1.0, device=rmse.device)/rmse)\n",
    "    return ps\n",
    "\n",
    "def save_rgb(path, img_tensor):\n",
    "    img = torch.clamp(img_tensor, 0, 1).permute(1,2,0).cpu().numpy()\n",
    "    img = (img*255.0+0.5).astype(np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, img)\n",
    "\n",
    "def is_image_file(filename):\n",
    "    filename = filename.lower()\n",
    "    return any(filename.endswith(ext) for ext in ['.jpeg','.jpg','.png','.gif','.bmp','.tif','.tiff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:52:44.515646Z",
     "iopub.status.busy": "2025-08-23T09:52:44.515449Z",
     "iopub.status.idle": "2025-08-23T09:52:44.531200Z",
     "shell.execute_reply": "2025-08-23T09:52:44.530651Z",
     "shell.execute_reply.started": "2025-08-23T09:52:44.515602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Denoising hyperparams\n",
    "from types import SimpleNamespace\n",
    "denoise_cfg = SimpleNamespace(\n",
    "    batch_size=4, num_epochs=1, lr=2e-4, lr_min=1e-6, train_ps=128, val_ps=256,\n",
    "    num_workers=2, seed=1234, session='mprnet_denoise'\n",
    ")\n",
    "# SR hyperparams (not used in this run)\n",
    "sr_cfg = SimpleNamespace(\n",
    "    scale=4, batch_size=4, num_epochs=1, lr=2e-4,\n",
    "    num_workers=2, seed=1234, session='edsr_small_x4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENOISE_TRAIN_DIR = /home/sachin/projects/DLP/deep-learning-practices/week-12/archive/train\n",
      "DENOISE_VAL_DIR   = /home/sachin/projects/DLP/deep-learning-practices/week-12/archive/val\n",
      "DENOISE_TEST_DIR  = /home/sachin/projects/DLP/deep-learning-practices/week-12/archive/test\n",
      "SUBMISSION_DIR    = /home/sachin/projects/DLP/deep-learning-practices/week-12/submission\n"
     ]
    }
   ],
   "source": [
    "# Local dataset paths under week-12 (using archive layout)\n",
    "DENOISE_ARCHIVE_ROOT = os.path.join(WEEK12_DIR, 'week-12', 'archive')\n",
    "DENOISE_TRAIN_DIR = os.path.join(DENOISE_ARCHIVE_ROOT, 'train')  # inputs under 'train/', targets under 'gt/'\n",
    "DENOISE_VAL_DIR   = os.path.join(DENOISE_ARCHIVE_ROOT, 'val')    # inputs under 'val/', targets under 'gt/'\n",
    "DENOISE_TEST_DIR  = os.path.join(DENOISE_ARCHIVE_ROOT, 'test')\n",
    "# SR placeholders (unused if not provided)\n",
    "SR_DATA_ROOT      = os.path.join(WEEK12_DIR, 'week-12', 'sr')\n",
    "SR_TRAIN_LR_DIR   = os.path.join(SR_DATA_ROOT, 'train', 'lr')\n",
    "SR_TRAIN_HR_DIR   = os.path.join(SR_DATA_ROOT, 'train', 'hr')\n",
    "SR_VAL_LR_DIR     = os.path.join(SR_DATA_ROOT, 'val', 'lr')\n",
    "SR_VAL_HR_DIR     = os.path.join(SR_DATA_ROOT, 'val', 'hr')\n",
    "SR_TEST_DIR       = os.path.join(SR_DATA_ROOT, 'test')\n",
    "SUBMISSION_DIR    = os.path.join(WEEK12_DIR, 'week-12', 'submission')\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "print('DENOISE_TRAIN_DIR =', DENOISE_TRAIN_DIR)\n",
    "print('DENOISE_VAL_DIR   =', DENOISE_VAL_DIR)\n",
    "print('DENOISE_TEST_DIR  =', DENOISE_TEST_DIR)\n",
    "print('SUBMISSION_DIR    =', SUBMISSION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising model: MPRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:52:44.532493Z",
     "iopub.status.busy": "2025-08-23T09:52:44.532253Z",
     "iopub.status.idle": "2025-08-23T09:52:44.571502Z",
     "shell.execute_reply": "2025-08-23T09:52:44.570834Z",
     "shell.execute_reply.started": "2025-08-23T09:52:44.532474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, bias=False, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias, stride=stride)\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, bias=False):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_du = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // reduction, 1, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel // reduction, channel, 1, bias=bias),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "\n",
    "class CAB(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, reduction, bias, act):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            conv(n_feat, n_feat, kernel_size, bias=bias),\n",
    "            act,\n",
    "            conv(n_feat, n_feat, kernel_size, bias=bias)\n",
    "        )\n",
    "        self.CA = CALayer(n_feat, reduction, bias=bias)\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res = self.CA(res)\n",
    "        return res + x\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, bias):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n",
    "        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n",
    "        self.conv3 = conv(3, n_feat, kernel_size, bias=bias)\n",
    "    def forward(self, x, x_img):\n",
    "        x1 = self.conv1(x)\n",
    "        img = self.conv2(x) + x_img\n",
    "        x2 = torch.sigmoid(self.conv3(img))\n",
    "        x1 = x1 * x2\n",
    "        x1 = x1 + x\n",
    "        return x1, img\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, s_factor):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False),\n",
    "                                nn.Conv2d(in_channels, in_channels + s_factor, 1, bias=False))\n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, s_factor):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                              nn.Conv2d(in_channels + s_factor, in_channels, 1, bias=False))\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class SkipUpSample(nn.Module):\n",
    "    def __init__(self, in_channels, s_factor):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                              nn.Conv2d(in_channels + s_factor, in_channels, 1, bias=False))\n",
    "    def forward(self, x, y):\n",
    "        x = self.up(x)\n",
    "        return x + y\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff):\n",
    "        super().__init__()\n",
    "        self.encoder_level1 = nn.Sequential(*[CAB(n_feat, kernel_size, reduction, bias=bias, act=act) for _ in range(2)])\n",
    "        self.encoder_level2 = nn.Sequential(*[CAB(n_feat+scale_unetfeats, kernel_size, reduction, bias=bias, act=act) for _ in range(2)])\n",
    "        self.encoder_level3 = nn.Sequential(*[CAB(n_feat+2*scale_unetfeats, kernel_size, reduction, bias=bias, act=act) for _ in range(2)])\n",
    "        self.down12  = DownSample(n_feat, scale_unetfeats)\n",
    "        self.down23  = DownSample(n_feat+scale_unetfeats, scale_unetfeats)\n",
    "        self.csff = csff\n",
    "        if csff:\n",
    "            self.csff_enc1 = nn.Conv2d(n_feat, n_feat, 1, bias=bias)\n",
    "            self.csff_enc2 = nn.Conv2d(n_feat+scale_unetfeats, n_feat+scale_unetfeats, 1, bias=bias)\n",
    "            self.csff_enc3 = nn.Conv2d(n_feat+2*scale_unetfeats, n_feat+2*scale_unetfeats, 1, bias=bias)\n",
    "            self.csff_dec1 = nn.Conv2d(n_feat, n_feat, 1, bias=bias)\n",
    "            self.csff_dec2 = nn.Conv2d(n_feat+scale_unetfeats, n_feat+scale_unetfeats, 1, bias=bias)\n",
    "            self.csff_dec3 = nn.Conv2d(n_feat+2*scale_unetfeats, n_feat+2*scale_unetfeats, 1, bias=bias)\n",
    "    def forward(self, x, encoder_outs=None, decoder_outs=None):\n",
    "        enc1 = self.encoder_level1(x)\n",
    "        if self.csff and (encoder_outs is not None) and (decoder_outs is not None):\n",
    "            enc1 = enc1 + self.csff_enc1(encoder_outs[0]) + self.csff_dec1(decoder_outs[0])\n",
    "        x = self.down12(enc1)\n",
    "        enc2 = self.encoder_level2(x)\n",
    "        if self.csff and (encoder_outs is not None) and (decoder_outs is not None):\n",
    "            enc2 = enc2 + self.csff_enc2(encoder_outs[1]) + self.csff_dec2(decoder_outs[1])\n",
    "        x = self.down23(enc2)\n",
    "        enc3 = self.encoder_level3(x)\n",
    "        if self.csff and (encoder_outs is not None) and (decoder_outs is not None):\n",
    "            enc3 = enc3 + self.csff_enc3(encoder_outs[2]) + self.csff_dec3(decoder_outs[2])\n",
    "        return [enc1, enc2, enc3]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats):\n",
    "        super().__init__()\n",
    "        self.decoder_level1 = nn.Sequential(*[CAB(n_feat, kernel_size, reduction, bias=bias, act=act) for _ in range(2)])\n",
    "        self.decoder_level2 = nn.Sequential(*[CAB(n_feat+scale_unetfeats, kernel_size, reduction, bias=bias, act=act) for _ in range(2)])\n",
    "        self.decoder_level3 = nn.Sequential(*[CAB(n_feat+2*scale_unetfeats, kernel_size, reduction, bias=bias, act=act) for _ in range(2)])\n",
    "        self.skip_attn1 = CAB(n_feat, kernel_size, reduction, bias=bias, act=act)\n",
    "        self.skip_attn2 = CAB(n_feat+scale_unetfeats, kernel_size, reduction, bias=bias, act=act)\n",
    "        self.up21  = SkipUpSample(n_feat, scale_unetfeats)\n",
    "        self.up32  = SkipUpSample(n_feat+scale_unetfeats, scale_unetfeats)\n",
    "    def forward(self, outs):\n",
    "        enc1, enc2, enc3 = outs\n",
    "        dec3 = self.decoder_level3(enc3)\n",
    "        x = self.up32(dec3, self.skip_attn2(enc2))\n",
    "        dec2 = self.decoder_level2(x)\n",
    "        x = self.up21(dec2, self.skip_attn1(enc1))\n",
    "        dec1 = self.decoder_level1(x)\n",
    "        return [dec1, dec2, dec3]\n",
    "\n",
    "class ORB(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, reduction, act, bias, num_cab):\n",
    "        super().__init__()\n",
    "        modules_body = [CAB(n_feat, kernel_size, reduction, bias=bias, act=act) for _ in range(num_cab)]\n",
    "        modules_body.append(conv(n_feat, n_feat, kernel_size))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        return res + x\n",
    "\n",
    "class ORSNet(nn.Module):\n",
    "    def __init__(self, n_feat, scale_orsnetfeats, kernel_size, reduction, act, bias, scale_unetfeats, num_cab):\n",
    "        super().__init__()\n",
    "        self.orb1 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n",
    "        self.orb2 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n",
    "        self.orb3 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n",
    "        self.up_enc1 = UpSample(n_feat, scale_unetfeats)\n",
    "        self.up_dec1 = UpSample(n_feat, scale_unetfeats)\n",
    "        self.up_enc2 = nn.Sequential(UpSample(n_feat+scale_unetfeats, scale_unetfeats), UpSample(n_feat, scale_unetfeats))\n",
    "        self.up_dec2 = nn.Sequential(UpSample(n_feat+scale_unetfeats, scale_unetfeats), UpSample(n_feat, scale_unetfeats))\n",
    "        self.conv_enc1 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, 1, bias=bias)\n",
    "        self.conv_enc2 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, 1, bias=bias)\n",
    "        self.conv_enc3 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, 1, bias=bias)\n",
    "        self.conv_dec1 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, 1, bias=bias)\n",
    "        self.conv_dec2 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, 1, bias=bias)\n",
    "        self.conv_dec3 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, 1, bias=bias)\n",
    "    def forward(self, x, encoder_outs, decoder_outs):\n",
    "        x = self.orb1(x)\n",
    "        x = x + self.conv_enc1(encoder_outs[0]) + self.conv_dec1(decoder_outs[0])\n",
    "        x = self.orb2(x)\n",
    "        x = x + self.conv_enc2(self.up_enc1(encoder_outs[1])) + self.conv_dec2(self.up_dec1(decoder_outs[1]))\n",
    "        x = self.orb3(x)\n",
    "        x = x + self.conv_enc3(self.up_enc2(encoder_outs[2])) + self.conv_dec3(self.up_dec2(decoder_outs[2]))\n",
    "        return x\n",
    "\n",
    "class MPRNet(nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3, n_feat=80, scale_unetfeats=48, scale_orsnetfeats=32, num_cab=8, kernel_size=3, reduction=4, bias=False):\n",
    "        super().__init__()\n",
    "        act = nn.PReLU()\n",
    "        self.shallow_feat1 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.shallow_feat2 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.shallow_feat3 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.stage1_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff=False)\n",
    "        self.stage1_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats)\n",
    "        self.stage2_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff=True)\n",
    "        self.stage2_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats)\n",
    "        self.stage3_orsnet = ORSNet(n_feat, scale_orsnetfeats, kernel_size, reduction, act, bias, scale_unetfeats, num_cab)\n",
    "        self.sam12 = SAM(n_feat, kernel_size=1, bias=bias)\n",
    "        self.sam23 = SAM(n_feat, kernel_size=1, bias=bias)\n",
    "        self.concat12 = conv(n_feat*2, n_feat, kernel_size, bias=bias)\n",
    "        self.concat23 = conv(n_feat*2, n_feat+scale_orsnetfeats, kernel_size, bias=bias)\n",
    "        self.tail = conv(n_feat+scale_orsnetfeats, out_c, kernel_size, bias=bias)\n",
    "    def forward(self, x3_img):\n",
    "        H, W = x3_img.size(2), x3_img.size(3)\n",
    "        x2top_img  = x3_img[:, :, 0:H//2, :]\n",
    "        x2bot_img  = x3_img[:, :, H//2:H, :]\n",
    "        x1ltop_img = x2top_img[:, :, :, 0:W//2]\n",
    "        x1rtop_img = x2top_img[:, :, :, W//2:W]\n",
    "        x1lbot_img = x2bot_img[:, :, :, 0:W//2]\n",
    "        x1rbot_img = x2bot_img[:, :, :, W//2:W]\n",
    "        x1ltop = self.shallow_feat1(x1ltop_img)\n",
    "        x1rtop = self.shallow_feat1(x1rtop_img)\n",
    "        x1lbot = self.shallow_feat1(x1lbot_img)\n",
    "        x1rbot = self.shallow_feat1(x1rbot_img)\n",
    "        feat1_ltop = self.stage1_encoder(x1ltop)\n",
    "        feat1_rtop = self.stage1_encoder(x1rtop)\n",
    "        feat1_lbot = self.stage1_encoder(x1lbot)\n",
    "        feat1_rbot = self.stage1_encoder(x1rbot)\n",
    "        feat1_top = [torch.cat((k, v), 3) for k, v in zip(feat1_ltop, feat1_rtop)]\n",
    "        feat1_bot = [torch.cat((k, v), 3) for k, v in zip(feat1_lbot, feat1_rbot)]\n",
    "        res1_top = self.stage1_decoder(feat1_top)\n",
    "        res1_bot = self.stage1_decoder(feat1_bot)\n",
    "        x2top_samfeats, stage1_img_top = self.sam12(res1_top[0], x2top_img)\n",
    "        x2bot_samfeats, stage1_img_bot = self.sam12(res1_bot[0], x2bot_img)\n",
    "        stage1_img = torch.cat([stage1_img_top, stage1_img_bot], 2)\n",
    "        x2top  = self.shallow_feat2(x2top_img)\n",
    "        x2bot  = self.shallow_feat2(x2bot_img)\n",
    "        x2top_cat = self.concat12(torch.cat([x2top, x2top_samfeats], 1))\n",
    "        x2bot_cat = self.concat12(torch.cat([x2bot, x2bot_samfeats], 1))\n",
    "        feat2_top = self.stage2_encoder(x2top_cat, feat1_top, res1_top)\n",
    "        feat2_bot = self.stage2_encoder(x2bot_cat, feat1_bot, res1_bot)\n",
    "        feat2 = [torch.cat((k, v), 2) for k, v in zip(feat2_top, feat2_bot)]\n",
    "        res2 = self.stage2_decoder(feat2)\n",
    "        x3_samfeats, stage2_img = self.sam23(res2[0], x3_img)\n",
    "        x3 = self.shallow_feat3(x3_img)\n",
    "        x3_cat = self.concat23(torch.cat([x3, x3_samfeats], 1))\n",
    "        x3_cat = self.stage3_orsnet(x3_cat, feat2, res2)\n",
    "        stage3_img = self.tail(x3_cat)\n",
    "        return [stage3_img + x3_img, stage2_img, stage1_img]\n",
    "\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, x, y):\n",
    "        diff = x - y\n",
    "        loss = torch.mean(torch.sqrt(diff * diff + self.eps * self.eps))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: Denoising and SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:54:32.578311Z",
     "iopub.status.busy": "2025-08-23T09:54:32.577547Z",
     "iopub.status.idle": "2025-08-23T09:54:32.599184Z",
     "shell.execute_reply": "2025-08-23T09:54:32.598422Z",
     "shell.execute_reply.started": "2025-08-23T09:54:32.578288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DenoiseTrainDataset(Dataset):\n",
    "    def __init__(self, root, patch_size=128):\n",
    "        super().__init__()\n",
    "        # Expect structure: root/train (inputs) and ../gt (targets)\n",
    "        self.inp_dir = os.path.join(root, 'train')\n",
    "        self.tar_dir = os.path.join(root, 'gt')\n",
    "        self.inp_files = sorted([f for f in os.listdir(self.inp_dir) if is_image_file(f)])\n",
    "        self.tar_files = sorted([f for f in os.listdir(self.tar_dir) if is_image_file(f)])\n",
    "        self.patch = patch_size\n",
    "    def __len__(self):\n",
    "        return min(len(self.inp_files), len(self.tar_files))\n",
    "    def __getitem__(self, idx):\n",
    "        inp_path = os.path.join(self.inp_dir, self.inp_files[idx])\n",
    "        tar_path = os.path.join(self.tar_dir, self.tar_files[idx])\n",
    "        inp_img = Image.open(inp_path).convert('RGB')\n",
    "        tar_img = Image.open(tar_path).convert('RGB')\n",
    "        # To tensor first\n",
    "        inp_t = TF.to_tensor(inp_img)\n",
    "        tar_t = TF.to_tensor(tar_img)\n",
    "        C, H, W = tar_t.shape\n",
    "        ps = int(self.patch) if self.patch else 0\n",
    "        # Replicate-pad tensors to ensure at least patch size\n",
    "        if ps:\n",
    "            pad_h = max(0, ps - H)\n",
    "            pad_w = max(0, ps - W)\n",
    "            if pad_h or pad_w:\n",
    "                # pad format (left, right, top, bottom)\n",
    "                inp_t = F.pad(inp_t, (0, pad_w, 0, pad_h), mode='replicate')\n",
    "                tar_t = F.pad(tar_t, (0, pad_w, 0, pad_h), mode='replicate')\n",
    "            _, H, W = tar_t.shape\n",
    "            # Safe random crop\n",
    "            if H > ps and W > ps:\n",
    "                rr = random.randint(0, H - ps)\n",
    "                cc = random.randint(0, W - ps)\n",
    "                inp_t = inp_t[:, rr:rr+ps, cc:cc+ps]\n",
    "                tar_t = tar_t[:, rr:rr+ps, cc:cc+ps]\n",
    "            else:\n",
    "                # Exactly ps or smaller handled by pad above; center crop to ps\n",
    "                inp_t = TF.center_crop(inp_t, (ps, ps))\n",
    "                tar_t = TF.center_crop(tar_t, (ps, ps))\n",
    "        fname = os.path.splitext(os.path.basename(tar_path))[0]\n",
    "        return tar_t, inp_t, fname\n",
    "\n",
    "class DenoiseValDataset(Dataset):\n",
    "    def __init__(self, root, patch_size=None):\n",
    "        super().__init__()\n",
    "        self.inp_dir = os.path.join(root, 'val')\n",
    "        self.tar_dir = os.path.join(root, 'gt')\n",
    "        self.inp_files = sorted([f for f in os.listdir(self.inp_dir) if is_image_file(f)])\n",
    "        self.tar_files = sorted([f for f in os.listdir(self.tar_dir) if is_image_file(f)])\n",
    "        self.ps = patch_size\n",
    "    def __len__(self):\n",
    "        return min(len(self.inp_files), len(self.tar_files))\n",
    "    def __getitem__(self, idx):\n",
    "        inp_path = os.path.join(self.inp_dir, self.inp_files[idx])\n",
    "        tar_path = os.path.join(self.tar_dir, self.tar_files[idx])\n",
    "        inp_img = Image.open(inp_path).convert('RGB')\n",
    "        tar_img = Image.open(tar_path).convert('RGB')\n",
    "        inp_t = TF.to_tensor(inp_img)\n",
    "        tar_t = TF.to_tensor(tar_img)\n",
    "        if self.ps:\n",
    "            C, H, W = tar_t.shape\n",
    "            ps = int(self.ps)\n",
    "            pad_h = max(0, ps - H)\n",
    "            pad_w = max(0, ps - W)\n",
    "            if pad_h or pad_w:\n",
    "                inp_t = F.pad(inp_t, (0, pad_w, 0, pad_h), mode='replicate')\n",
    "                tar_t = F.pad(tar_t, (0, pad_w, 0, pad_h), mode='replicate')\n",
    "            inp_t = TF.center_crop(inp_t, (ps, ps))\n",
    "            tar_t = TF.center_crop(tar_t, (ps, ps))\n",
    "        fname = os.path.splitext(os.path.basename(tar_path))[0]\n",
    "        return tar_t, inp_t, fname\n",
    "\n",
    "class DenoiseTestDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        super().__init__()\n",
    "        self.files = sorted([f for f in os.listdir(root) if is_image_file(f)])\n",
    "        self.root = root\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root, self.files[idx])\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return TF.to_tensor(img), os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir=None, hr_dir=None, scale=4, synthesize_if_missing=False, split='train'):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.synthesize = synthesize_if_missing or (not (lr_dir and os.path.isdir(lr_dir)))\n",
    "        if self.synthesize:\n",
    "            self.hr_files = sorted([f for f in os.listdir(hr_dir) if is_image_file(f)]) if hr_dir and os.path.isdir(hr_dir) else []\n",
    "        else:\n",
    "            self.lr_files = sorted([f for f in os.listdir(lr_dir) if is_image_file(f)])\n",
    "            self.hr_files = sorted([f for f in os.listdir(hr_dir) if is_image_file(f)])\n",
    "        self.split = split\n",
    "    def __len__(self):\n",
    "        return len(self.hr_files) if self.synthesize else min(len(self.lr_files), len(self.hr_files))\n",
    "    def __getitem__(self, idx):\n",
    "        if self.synthesize:\n",
    "            hr_path = os.path.join(self.hr_dir, self.hr_files[idx])\n",
    "            hr = Image.open(hr_path).convert('RGB')\n",
    "            w, h = hr.size\n",
    "            lr = hr.resize((max(1,w//self.scale), max(1,h//self.scale)), Image.BICUBIC)\n",
    "        else:\n",
    "            lr_path = os.path.join(self.lr_dir, self.lr_files[idx])\n",
    "            hr_path = os.path.join(self.hr_dir, self.hr_files[idx])\n",
    "            lr = Image.open(lr_path).convert('RGB')\n",
    "            hr = Image.open(hr_path).convert('RGB')\n",
    "        lr_t = TF.to_tensor(lr)\n",
    "        hr_t = TF.to_tensor(hr)\n",
    "        name = os.path.splitext(self.hr_files[idx] if self.synthesize else self.hr_files[idx])[0]\n",
    "        return hr_t, lr_t, name\n",
    "\n",
    "class SRTestDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        super().__init__()\n",
    "        self.files = sorted([f for f in os.listdir(root) if is_image_file(f)])\n",
    "        self.root = root\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root, self.files[idx])\n",
    "        lr = Image.open(path).convert('RGB')\n",
    "        return TF.to_tensor(lr), os.path.splitext(os.path.basename(path))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train denoiser (MPRNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T09:54:51.653044Z",
     "iopub.status.busy": "2025-08-23T09:54:51.652481Z",
     "iopub.status.idle": "2025-08-23T09:54:52.278453Z",
     "shell.execute_reply": "2025-08-23T09:54:52.276986Z",
     "shell.execute_reply.started": "2025-08-23T09:54:51.653023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [denoise]:   0%|          | 0/1105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [denoise]:   2%|â–         | 18/1105 [00:00<00:12, 86.06it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training for denoiser (robust to variable image sizes)\n",
    "import os, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fallback loss if CharbonnierLoss isn't defined earlier\n",
    "class _L1Like(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.L1Loss()\n",
    "    def forward(self, x, y):\n",
    "        return self.loss(x, y)\n",
    "\n",
    "\n",
    "def _get_device():\n",
    "    d = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        _ = torch.tensor([0.0]).to(d)\n",
    "        return d\n",
    "    except Exception:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def _round_up(x, m):\n",
    "    return int(math.ceil(x / m) * m)\n",
    "\n",
    "\n",
    "def _prepare_tensor(x: torch.Tensor, min_hw: int = 64, factor: int = 32):\n",
    "    # Ensure H,W >= min_hw and multiples of factor using bilinear upsample (safe for tiny images)\n",
    "    B, C, H, W = x.shape\n",
    "    if H == 0 or W == 0:\n",
    "        raise RuntimeError(f'Encountered zero-sized tensor with shape {tuple(x.shape)}')\n",
    "    target_h = max(min_hw, _round_up(H, factor))\n",
    "    target_w = max(min_hw, _round_up(W, factor))\n",
    "    if target_h == H and target_w == W:\n",
    "        return x, (H, W)\n",
    "    x_up = F.interpolate(x, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
    "    return x_up, (H, W)\n",
    "\n",
    "\n",
    "def _resize_to(x: torch.Tensor, size_hw: tuple):\n",
    "    H, W = size_hw\n",
    "    H = int(max(1, H)); W = int(max(1, W))\n",
    "    if x.shape[-2:] == (H, W):\n",
    "        return x\n",
    "    return F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "def train_denoiser(train_dir, val_dir, cfg):\n",
    "    global WORK_DIR\n",
    "    device = _get_device()\n",
    "\n",
    "    # Datasets defined earlier in the notebook\n",
    "    train_ds = DenoiseTrainDataset(train_dir)\n",
    "    val_ds = DenoiseValDataset(val_dir, patch_size=None)  # avoid forced center-crop size\n",
    "\n",
    "    # Use batch_size=1 to avoid size mismatch across samples\n",
    "    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = MPRNet().to(device)\n",
    "\n",
    "    # Try to use CharbonnierLoss if available else L1\n",
    "    criterion = CharbonnierLoss() if 'CharbonnierLoss' in globals() else _L1Like()\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=float(cfg.lr))\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=int(cfg.num_epochs))\n",
    "\n",
    "    best_psnr = -1.0\n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    best_ckpt = os.path.join(WORK_DIR, 'best_denoiser.pth')\n",
    "\n",
    "    for epoch in range(1, int(cfg.num_epochs) + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{cfg.num_epochs} [denoise]')\n",
    "        for tar, inp, _ in pbar:\n",
    "            tar, inp = tar.to(device), inp.to(device)\n",
    "            # Ensure no zero-sized tensors\n",
    "            if tar.numel() == 0 or inp.numel() == 0 or tar.shape[-1] == 0 or tar.shape[-2] == 0 or inp.shape[-1] == 0 or inp.shape[-2] == 0:\n",
    "                continue\n",
    "            # Upsample to safe sizes (>=64, multiples of 32) for MPRNet tiling\n",
    "            inp_proc, _ = _prepare_tensor(inp, min_hw=64, factor=32)\n",
    "            # For loss, resize output back to target size\n",
    "            optimizer.zero_grad()\n",
    "            out = model(inp_proc)\n",
    "            out = _resize_to(out, (tar.shape[-2], tar.shape[-1]))\n",
    "            loss = criterion(out, tar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            psnrs = []\n",
    "            for tar, inp, _ in val_loader:\n",
    "                tar, inp = tar.to(device), inp.to(device)\n",
    "                if tar.numel() == 0 or inp.numel() == 0 or tar.shape[-1] == 0 or tar.shape[-2] == 0 or inp.shape[-1] == 0 or inp.shape[-2] == 0:\n",
    "                    continue\n",
    "                inp_proc, _ = _prepare_tensor(inp, min_hw=64, factor=32)\n",
    "                pred = model(inp_proc)\n",
    "                pred = _resize_to(pred, (tar.shape[-2], tar.shape[-1]))\n",
    "                pred = torch.clamp(pred, 0.0, 1.0)\n",
    "                mse = torch.mean((pred - tar) ** 2).item()\n",
    "                psnr = 100.0 if mse == 0 else 10 * math.log10(1.0 / mse)\n",
    "                psnrs.append(psnr)\n",
    "            val_psnr = sum(psnrs) / max(1, len(psnrs))\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss={epoch_loss/max(1,len(train_loader)):.4f}, val_psnr={val_psnr:.2f}dB\")\n",
    "\n",
    "        if val_psnr > best_psnr:\n",
    "            best_psnr = val_psnr\n",
    "            torch.save({'model': model.state_dict(), 'val_psnr': val_psnr, 'epoch': epoch}, best_ckpt)\n",
    "            print(f\"Saved new best denoiser to {best_ckpt} (PSNR {val_psnr:.2f}dB)\")\n",
    "\n",
    "    return best_ckpt\n",
    "\n",
    "# Train using local week-12 data\n",
    "try:\n",
    "    denoise_ckpt = train_denoiser(DENOISE_TRAIN_DIR, DENOISE_VAL_DIR, denoise_cfg)\n",
    "    print('Saved best denoiser ckpt at:', denoise_ckpt)\n",
    "except RuntimeError as e:\n",
    "    print('Encountered RuntimeError, retrying on CPU. Error was:', str(e))\n",
    "    torch.cuda.is_available = lambda: False\n",
    "    denoise_ckpt = train_denoiser(DENOISE_TRAIN_DIR, DENOISE_VAL_DIR, denoise_cfg)\n",
    "    print('Saved best denoiser ckpt at:', denoise_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4x Super-Resolution model: Compact EDSR-like network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_feats, res_scale=0.1):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        )\n",
    "        self.res_scale = res_scale\n",
    "    def forward(self, x):\n",
    "        res = self.body(x).mul(self.res_scale)\n",
    "        return x + res\n",
    "\n",
    "class EDSRSmall(nn.Module):\n",
    "    def __init__(self, scale=4, n_resblocks=8, n_feats=64):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(3, n_feats, 3, 1, 1)\n",
    "        self.body = nn.Sequential(*[ResBlock(n_feats) for _ in range(n_resblocks)])\n",
    "        # Upsampler to 4x via two PixelShuffle x2\n",
    "        up = []\n",
    "        for _ in range(int(math.log2(scale))):\n",
    "            up += [nn.Conv2d(n_feats, n_feats*4, 3, 1, 1), nn.PixelShuffle(2), nn.ReLU(inplace=True)]\n",
    "        self.upsample = nn.Sequential(*up)\n",
    "        self.tail = nn.Conv2d(n_feats, 3, 3, 1, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = self.body(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.tail(x)\n",
    "        return x\n",
    "\n",
    "def train_sr(train_lr_dir, train_hr_dir, val_lr_dir, val_hr_dir, cfg):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    set_seed(cfg.seed)\n",
    "    model = EDSRSmall(scale=cfg.scale).to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.num_epochs, eta_min=1e-6)\n",
    "    synth = not (train_lr_dir and os.path.isdir(train_lr_dir))\n",
    "    tr_ds = SRDataset(lr_dir=train_lr_dir, hr_dir=train_hr_dir, scale=cfg.scale, synthesize_if_missing=synth, split='train')\n",
    "    va_ds = SRDataset(lr_dir=val_lr_dir, hr_dir=val_hr_dir, scale=cfg.scale, synthesize_if_missing=(not (val_lr_dir and os.path.isdir(val_lr_dir))), split='val')\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    va_loader = DataLoader(va_ds, batch_size=1, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    best_psnr, best_epoch = -1.0, -1\n",
    "    ckpt_best = os.path.join('.', f'sr_{cfg.session}_best.pth')\n",
    "    for epoch in range(1, cfg.num_epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for hr, lr, _ in tqdm(tr_loader, desc=f'Epoch {epoch}/{cfg.num_epochs} [sr]'):\n",
    "            hr, lr = hr.to(device), lr.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            sr = model(lr)\n",
    "            # Ensure size match: center-crop/pad if minor mismatch\n",
    "            _, _, Hh, Wh = hr.shape\n",
    "            _, _, Hs, Ws = sr.shape\n",
    "            H = min(Hh, Hs); W = min(Wh, Ws)\n",
    "            sr_crop = sr[:, :, :H, :W]\n",
    "            hr_crop = hr[:, :, :H, :W]\n",
    "            loss = criterion(sr_crop, hr_crop)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        psnrs = []\n",
    "        with torch.no_grad():\n",
    "            for hr, lr, _ in va_loader:\n",
    "                hr, lr = hr.to(device), lr.to(device)\n",
    "                sr = model(lr)\n",
    "                _, _, Hh, Wh = hr.shape\n",
    "                _, _, Hs, Ws = sr.shape\n",
    "                H = min(Hh, Hs); W = min(Wh, Ws)\n",
    "                psnrs.append(torch_psnr(hr[:, :, :H, :W], sr[:, :, :H, :W]).item())\n",
    "        mean_psnr = float(np.mean(psnrs)) if psnrs else float('nan')\n",
    "        print(f'Epoch {epoch}: loss={epoch_loss/len(tr_loader):.4f} val_psnr={mean_psnr:.3f}')\n",
    "        if mean_psnr > best_psnr:\n",
    "            best_psnr, best_epoch = mean_psnr, epoch\n",
    "            torch.save({'epoch': epoch, 'state_dict': model.state_dict()}, ckpt_best)\n",
    "    print(f'Best SR PSNR: {best_psnr:.3f} @ epoch {best_epoch}')\n",
    "    return ckpt_best\n",
    "\n",
    "print('SR train HR exists:', os.path.isdir(SR_TRAIN_HR_DIR))\n",
    "print('SR train LR exists:', os.path.isdir(SR_TRAIN_LR_DIR))\n",
    "To train now, uncomment:\n",
    "sr_ckpt = train_sr(SR_TRAIN_LR_DIR, SR_TRAIN_HR_DIR, SR_VAL_LR_DIR, SR_VAL_HR_DIR, sr_cfg)\n",
    "print('Saved best SR ckpt at:', sr_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pipeline (Denoise -> 4x SR) and create submission.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def _extract_state_dict(ckpt):\n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'state_dict' in ckpt:\n",
    "            return ckpt['state_dict']\n",
    "        if 'model' in ckpt:\n",
    "            return ckpt['model']\n",
    "    return ckpt\n",
    "\n",
    "def load_denoiser(ckpt_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = MPRNet().to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state = _extract_state_dict(ckpt)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_sr(ckpt_path, scale=4):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = EDSRSmall(scale=scale).to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state = _extract_state_dict(ckpt)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def run_pipeline_and_save(denoise_ckpt, sr_ckpt, test_denoise_dir, out_dir, scale=4):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    denoiser = load_denoiser(denoise_ckpt)\n",
    "    srnet   = load_sr(sr_ckpt, scale=scale)\n",
    "    ds = DenoiseTestDataset(test_denoise_dir)\n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=1)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for inp, name in tqdm(dl, desc='Infer test'):\n",
    "            inp = inp.to(device)\n",
    "            den = denoiser(inp)[0]\n",
    "            sr  = srnet(den)\n",
    "            # Save\n",
    "            save_rgb(os.path.join(out_dir, f'{name[0]}.png'), sr.squeeze(0).cpu())\n",
    "    # Zip for Kaggle submission\n",
    "    zip_path = os.path.join(WORK_DIR, 'submission.zip')\n",
    "    with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for fn in sorted(os.listdir(out_dir)):\n",
    "            if fn.lower().endswith('.png'):\n",
    "                zf.write(os.path.join(out_dir, fn), arcname=fn)\n",
    "    print('Submission zipped at:', zip_path)\n",
    "    return zip_path\n",
    "\n",
    "Example usage after training:\n",
    "denoise_ckpt = os.path.join('.', f'denoise_{denoise_cfg.session}_best.pth')\n",
    "sr_ckpt      = os.path.join('.', f'sr_{sr_cfg.session}_best.pth')\n",
    "submission_zip = run_pipeline_and_save(denoise_ckpt, sr_ckpt, DENOISE_TEST_DIR, SUBMISSION_DIR, scale=sr_cfg.scale)\n",
    "submission_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Validation PSNR of the full pipeline (if HR val available)\n",
    "If you have LR/HR validation pairs for SR and input/target pairs for denoising, you can compute PSNR of denoised+SR outputs against HR targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_pipeline_psnr(denoise_ckpt, sr_ckpt, denoise_val_dir, sr_val_lr_dir, sr_val_hr_dir, max_samples=20):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    denoiser = load_denoiser(denoise_ckpt)\n",
    "    srnet   = load_sr(sr_ckpt, scale=sr_cfg.scale)\n",
    "    # Denoise val inputs\n",
    "    den_val_ds = DenoiseValDataset(denoise_val_dir, patch_size=None)\n",
    "    sr_val_ds  = SRDataset(lr_dir=sr_val_lr_dir, hr_dir=sr_val_hr_dir, scale=sr_cfg.scale, synthesize_if_missing=(not os.path.isdir(sr_val_lr_dir)), split='val')\n",
    "    n = min(len(den_val_ds), len(sr_val_ds), max_samples)\n",
    "    psnrs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            tar_dn, inp_dn, _ = den_val_ds[i]\n",
    "            hr, lr, _ = sr_val_ds[i]\n",
    "            den = denoiser(inp_dn.unsqueeze(0).to(device))[0]\n",
    "            sr  = srnet(den)\n",
    "            H = min(hr.shape[1], sr.shape[2]); W = min(hr.shape[2], sr.shape[3])\n",
    "            psnrs.append(torch_psnr(hr[:, :H, :W].unsqueeze(0).to(device), sr[:, :, :H, :W]).item())\n",
    "    print('Pipeline val PSNR (approx):', np.mean(psnrs) if psnrs else float('nan'))\n",
    "# Example:\n",
    "# validate_pipeline_psnr(denoise_ckpt, sr_ckpt, DENOISE_VAL_DIR, SR_VAL_LR_DIR, SR_VAL_HR_DIR)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13352526,
     "sourceId": 112035,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pytorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
