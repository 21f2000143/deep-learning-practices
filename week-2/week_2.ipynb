{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ba73bd",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055dc423",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd339a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smlab\\.conda\\envs\\dlpenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b261062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3338ad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 74004228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('bookcorpus', split='all')\n",
    "pprint(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f304b",
   "metadata": {},
   "source": [
    "## Print some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f475bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : usually , he would be tearing around the living room , playing with his toys .\n",
      "1 : but just one look at a minion sent him practically catatonic .\n",
      "2 : that had been megan 's plan when she got him dressed earlier .\n",
      "3 : he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\n",
      "4 : she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .\n",
      "5 : `` are n't you being a good boy ? ''\n"
     ]
    }
   ],
   "source": [
    "num_samples = 6\n",
    "for idx, sample in enumerate(ds[0:num_samples]['text']):\n",
    "  print(f'{idx} : {sample}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b88e",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827a865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a49f1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPE(unk_token=\"[UNK]\")\n",
    "tokenizer = Tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f36c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d585f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(vocab_size=32000, special_tokens=['[PAD]', '[UNK]'], continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa23c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "print(cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d944a75",
   "metadata": {},
   "source": [
    "## Now the pipeline is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9372b",
   "metadata": {},
   "source": [
    "## Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37e98e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(batch_size=1000):\n",
    "  for i in range(0, len(ds), batch_size):\n",
    "    yield ds[i : i + batch_size]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc004e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer, length=len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b0238",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a3d7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model\\\\hopper-vocab.json', 'model\\\\hopper-merges.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('model', prefix='hopper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90476555",
   "metadata": {},
   "source": [
    "## Display last n merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fad62e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel ##anthe\n",
      "\n",
      "black ##er\n",
      "\n",
      "ad ##ject\n",
      "\n",
      "v ##ang\n",
      "\n",
      "betroth ##al\n",
      "\n",
      "tiptoe ##ing\n",
      "\n",
      "restroom ##s\n",
      "\n",
      "consol ##ing\n",
      "\n",
      "esp ##ionage\n",
      "\n",
      "influ ##x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('model/hopper-merges.txt', 'r') as file:\n",
    "  row = 0\n",
    "  num_lines = 10\n",
    "  for line in reversed(file.readlines()):\n",
    "    print(line)\n",
    "    row+=1\n",
    "    if row >= num_lines:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88632f9c",
   "metadata": {},
   "source": [
    "## View the number of merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f736ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/hopper-merges.txt', 'r') as file:\n",
    "  lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d42eec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merges: 31871\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of merges: {len(lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3887e24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2f6ae",
   "metadata": {},
   "source": [
    "## Get the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b8f03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a96f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e90a7",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5ca7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: usually , he would be tearing around the living room , playing with his toys .\n",
      "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "sample = ds[0]['text']\n",
    "print(f'sample: {sample}')\n",
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3a06fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = encoding.ids\n",
    "tokens = encoding.tokens\n",
    "type_ids = encoding.type_ids\n",
    "attention_mask = encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94706d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Hello, notebook!</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<b>Hello, notebook!</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ef4b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5cbb392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >usually</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >he</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >would</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >be</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tearing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >around</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >living</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >room</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >playing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >his</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >toys</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "visualizer = EncodingVisualizer(tokenizer=tokenizer, default_to_notebook=False)\n",
    "html = visualizer(text=sample)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658d4c8",
   "metadata": {},
   "source": [
    "## Let's try to understand the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b3a15db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ids</th>\n",
       "      <th>type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually</td>\n",
       "      <td>2462</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tearing</td>\n",
       "      <td>6456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>around</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>living</td>\n",
       "      <td>1559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>room</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>playing</td>\n",
       "      <td>2301</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>with</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>his</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>toys</td>\n",
       "      <td>9774</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens   ids  type_ids  attention_mask\n",
       "0   usually  2462         0               1\n",
       "1         ,    19         0               1\n",
       "2        he   149         0               1\n",
       "3     would   277         0               1\n",
       "4        be   162         0               1\n",
       "5   tearing  6456         0               1\n",
       "6    around   422         0               1\n",
       "7       the   131         0               1\n",
       "8    living  1559         0               1\n",
       "9      room   536         0               1\n",
       "10        ,    19         0               1\n",
       "11  playing  2301         0               1\n",
       "12     with   201         0               1\n",
       "13      his   177         0               1\n",
       "14     toys  9774         0               1\n",
       "15        .    21         0               1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict = {'tokens': tokens, 'ids':token_ids, 'type_ids':type_ids, 'attention_mask':attention_mask}\n",
    "df = pd.DataFrame(out_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52262a5f",
   "metadata": {},
   "source": [
    "## Batch Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4466445",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ds[0:4]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd8472f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "pprint(batch_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fabf4d",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1de00aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all default args\n",
    "tokenizer.enable_padding(direction='right',\n",
    "                         pad_id=0,\n",
    "                         pad_type_id=0,\n",
    "                         pad_token = '[PAD]',\n",
    "                         length=None, # None default to max_len in the batch\n",
    "                         pad_to_multiple_of = None)\n",
    "\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd64e5",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3515955d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'this', 'is', 'so', 'simple', 'to', 'do', 'in', 'h', '##f', '%', '[UNK]', '##.']\n"
     ]
    }
   ],
   "source": [
    "text = 'All this is so simple to do in HF %$.'\n",
    "encoded = tokenizer.encode(text).tokens\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2003c4b8",
   "metadata": {},
   "source": [
    "### Applying encoding with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e32b2491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "pprint(batch_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7578899",
   "metadata": {},
   "source": [
    "## Save the model with every params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4c26fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('awesome_tokenizer\\\\tokenizer_config.json',\n",
       " 'awesome_tokenizer\\\\special_tokens_map.json',\n",
       " 'awesome_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.save('hopper.json')\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "awesome_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "awesome_tokenizer.save_pretrained(\"awesome_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bc0da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('hopper.json', 'r') as file:\n",
    "  json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2805d06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'added_tokens': [...],\n",
      " 'decoder': None,\n",
      " 'model': {...},\n",
      " 'normalizer': {...},\n",
      " 'padding': {...},\n",
      " 'post_processor': None,\n",
      " 'pre_tokenizer': {...},\n",
      " 'truncation': {...},\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "pprint(json_data, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d31f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ca93ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf423b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_tokenizer = trained_tokenizer.from_file('hopper.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f69f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'this', 'is', 'so', 'simple', 'to', 'do', 'in', 'h', '##f', '%', '[UNK]', '##.']\n"
     ]
    }
   ],
   "source": [
    "tokens = trained_tokenizer.encode(text).tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb35f4",
   "metadata": {},
   "source": [
    "## BERT-like Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8e60f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "bert_tokenizer.normalizer = Lowercase()\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "bert_trainer = BpeTrainer(vocab_size=32000,\n",
    "                          special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'],\n",
    "                          continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce037b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b536c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.post_processor = TemplateProcessing(single='[CLS] $0 [SEP]',\n",
    "                                                   pair=\"[CLS] $A [SEP] $B:1\",\n",
    "                                                   special_tokens=[('[CLS]', 2), ('[SEP]', 3)],\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23542929",
   "metadata": {},
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dae44761",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=bert_trainer, length=len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c127d",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ba8b5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model\\\\bert-vocab.json', 'model\\\\bert-merges.txt']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.model.save('model', prefix='bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314efda2",
   "metadata": {},
   "source": [
    "## Test the BERT-like tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e6ba1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [2, 270, 956, 336, 231, 2534, 141, 206, 157, 56, 98, 24, 462, 17, 67,\n",
      "         206, 387, 3],\n",
      " 'tokens': ['[CLS]', 'all', 'these', 'are', 'so', 'simple', 'to', 'do', 'in',\n",
      "            'h', '##f', '.', 'let', \"'\", 's', 'do', 'more', '[SEP]']}\n"
     ]
    }
   ],
   "source": [
    "text = \"All these are so simple to do in HF. Let's do more\"\n",
    "encoded = bert_tokenizer.encode(text)\n",
    "tokens = encoded.tokens\n",
    "ids = encoded.ids\n",
    "out_dict = {'tokens': tokens, 'ids': ids}\n",
    "pprint(out_dict, depth=2, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b8e5f",
   "metadata": {},
   "source": [
    "### For a pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "545aed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [2, 270, 956, 336, 231, 2534, 141, 206, 157, 56, 98, 24, 462, 17, 67,\n",
      "         206, 387, 3, 214, 250, 49, 490, 415, 141, 260, 12],\n",
      " 'tokens': ['[CLS]', 'all', 'these', 'are', 'so', 'simple', 'to', 'do', 'in',\n",
      "            'h', '##f', '.', 'let', \"'\", 's', 'do', 'more', '[SEP]', 'we',\n",
      "            'have', 'a', 'long', 'way', 'to', 'go', '!']}\n"
     ]
    }
   ],
   "source": [
    "text = \"All these are so simple to do in HF. Let's do more\"\n",
    "pair = \"We have a long way to go!\"\n",
    "encoded = bert_tokenizer.encode(text, pair)\n",
    "tokens = encoded.tokens\n",
    "ids = encoded.ids\n",
    "out_dict = {'tokens': tokens, 'ids': ids}\n",
    "pprint(out_dict, depth=2, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af788d30",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bf95d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all these are so simple to do in h ##f . let ' s do more we have a long way to go !\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_tokens = bert_tokenizer.decode(ids)\n",
    "plain_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e72f3d",
   "metadata": {},
   "source": [
    "## Appropriate Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b60f4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d00a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decoder = WordPiece(prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61ce23da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all these are so simple to do in hf. let ' s do more we have a long way to go!\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_tokens =  bert_tokenizer.decode(ids)\n",
    "plain_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f8f68",
   "metadata": {},
   "source": [
    "## Pre Trained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "114feebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8a03ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokenizer = PreTrainedTokenizerFast(tokenizer_file='hopper.json',\n",
    "                                       unk_token='[UNK]',\n",
    "                                       pad_token='[PAD]',\n",
    "                                       model_input_names=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644267ff",
   "metadata": {},
   "source": [
    "# Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc11b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [267, 953, 333, 228, 2531, 138, 203, 154, 53, 95, 21, 459, 14, 64,\n",
      "               203, 384],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = pt_tokenizer(text)\n",
    "pprint(model_inputs, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "101eed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1],\n",
      " 'input_ids': [267, 953, 333, 228, 2531, 138, 203, 154, 53, 95, 21, 459, 14, 64,\n",
      "               203, 384, 211, 247, 46, 487, 412, 138, 257, 9],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = pt_tokenizer(text, text_pair=pair)\n",
    "pprint(model_inputs, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef21853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_text = ['I like the book The Psychology of Money', 'I enjoyed watching the Transformers movie', 'oh! thanks for this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa296061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[54, 281, 131, 1701, 131, 19478, 153, 1564],\n",
      "               [54, 4096, 1443, 131, 7744, 307, 3760],\n",
      "               [772, 9, 1767, 200, 254]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = pt_tokenizer(batch_text)\n",
    "pprint(model_inputs, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5a96731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "                    [1, 1, 1, 1, 1, 0, 0, 0]],\n",
      " 'input_ids': [[54, 281, 131, 1701, 131, 19478, 153, 1564],\n",
      "               [54, 4096, 1443, 131, 7744, 307, 3760, 0],\n",
      "               [772, 9, 1767, 200, 254, 0, 0, 0]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# With padding\n",
    "model_inputs = pt_tokenizer(batch_text, padding=True)\n",
    "pprint(model_inputs, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4ac37",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c290f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Layout, interact, interactive, fixed, interact_manual, widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29279808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "  pairs = collections.defaultdict(int)\n",
    "  for word, freq in vocab.items():\n",
    "    symbols = word.split()\n",
    "    for i in range(len(symbols)-1):\n",
    "      pairs[symbols[i],symbols[i+1]] += freq\n",
    "  return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "  v_out = {}\n",
    "  bigram = re.escape(' '.join(pair))\n",
    "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "  for word in v_in:\n",
    "  \tw_out = p.sub(''.join(pair), word)\n",
    "  \tv_out[w_out] = v_in[word]\n",
    "  return v_out\n",
    "\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n",
    "'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "  pairs = get_stats(vocab)\n",
    "  best = max(pairs, key=pairs.get)\n",
    "  vocab = merge_vocab(best, vocab)\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37aaa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display a pair of subtokens to be merged in a slider\n",
    "def get_pairs(pair:int):\n",
    "    \"\"\"\n",
    "    pair: index of the pair. \n",
    "    \"\"\"\n",
    "    if pair>0:\n",
    "        left, right = lines[pair].strip('\\n').split(' ')\n",
    "        print(f'{left} , {right}')\n",
    "        \n",
    "# to display token ids  in a slider\n",
    "def display_token_id(id):\n",
    "    token,id = vocab_sorted[id]\n",
    "    print(f'id:{id} \\t token:{token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5a81d",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ac0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e52c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (/home/sachin/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 74004228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "subset = load_dataset('bookcorpus',split='all')\n",
    "pprint(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53caa55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset.select(range(0, len(subset), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f3c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10572033\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2776f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['usually , he would be tearing around the living room , playing with his toys .',\n",
       "  'mason barely acknowledged her .',\n",
       "  'mason was already registering off the charts in height and weight according to his pediatrician .',\n",
       "  'she never wanted anything in the world to hurt him , and she knew that being rejected by his father would .',\n",
       "  \"aidan was her mother 's baby brother and only son of the family .\",\n",
       "  \"while it had been no question that she wanted him as godfather for mason , she had been extremely honored when he and his wife , emma , had asked her to be their son , noah 's , godmother .\"]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "732f491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1da8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPE(unk_token = '[UNK]')\n",
    "tokenizer = Tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b626a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2466b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "def trainer_with_vocab_size(vocab_size=10000):\n",
    "  trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=['GO', 'UNK', 'PAD', 'EOS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b6b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(batch_size=1000):\n",
    "  for i in range(0, len(subset), batch_size):\n",
    "    yield subset[i: i+batch_size]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6802079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer with the vocab_size \n",
    "trainer = trainer_with_vocab_size(vocab_size=5000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer, length=len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff618fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('hopper5k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aafb0d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce601c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer10 = trainer_with_vocab_size(vocab_size=10000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer10, length=len(subset))\n",
    "tokenizer.save('hopper10k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03d735c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer15 = trainer_with_vocab_size(vocab_size=15000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer15, length=len(subset))\n",
    "tokenizer.save('hopper15k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d7132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer32 = trainer_with_vocab_size(vocab_size=32000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer32, length=len(subset))\n",
    "tokenizer.save('hopper32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "524ee28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())\n",
    "trained_tokenizer = trained_tokenizer.from_file('hopper5k.json')\n",
    "tokens = trained_tokenizer.encode(input_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aeae3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())\n",
    "trained_tokenizer = trained_tokenizer.from_file('hopper10k.json')\n",
    "tokens = trained_tokenizer.encode(input_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db6a97ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=25, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())\n",
    "trained_tokenizer = trained_tokenizer.from_file('hopper.json')\n",
    "tokens = trained_tokenizer.encode(input_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a513db4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.models.BPE at 0x7df0cb11b070>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.from_file('./model/hopper10-vocab.json', 'model/hopper10-merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57eed68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80fc6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.models.BPE' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model/hopper15-vocab.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/hopper15-merges.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEBI study finds 93\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mf individual F&O traders made losses between FY22 and FY24.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(input_text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.models.BPE' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "trainer10 = trainer_with_vocab_size(vocab_size=15000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer10, length=len(subset))\n",
    "tokenizer.model.save('model', prefix='hopper15')\n",
    "tokenizer = tokenizer.model.from_file('./model/hopper15-vocab.json', 'model/hopper15-merges.txt')\n",
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617587fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Token count: Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trainer10 = trainer_with_vocab_size(vocab_size=32000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer10, length=len(subset))\n",
    "tokenizer.model.save('model', prefix='hopper32')\n",
    "tokenizer = tokenizer.model.from_file('./model/hopper32-vocab.json', 'model/hopper32-merges.txt')\n",
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff48c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=5000,  # Change to 10000, 15000, 32000 as needed\n",
    "    special_tokens=[\"[GO]\", \"[UNK]\", \"[PAD]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "# Suppose `dataset` is a list of 10,572,033 strings\n",
    "# dataset = load_bookcorpus_every_7th_sample()\n",
    "tokenizer.train_from_iterator(subset, trainer)\n",
    "\n",
    "# Save and reload for reuse\n",
    "tokenizer.save(\"custom_bpe_5000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b2ce758",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m----> 3\u001b[0m hopper_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhopper.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m hopper_tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens (hopper):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens\u001b[38;5;241m.\u001b[39mtokens))\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "hopper_tokenizer = Tokenizer.from_file(\"hopper.json\")\n",
    "tokens = hopper_tokenizer.encode(input_text)\n",
    "print(\"Tokens (hopper):\", len(tokens.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75351b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hopper_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhopper_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39madd_tokens([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFY\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m tokens_after \u001b[38;5;241m=\u001b[39m hopper_tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens after adding FY:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens_after\u001b[38;5;241m.\u001b[39mtokens))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hopper_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "hopper_tokenizer.model.add_tokens([\"FY\"])\n",
    "tokens_after = hopper_tokenizer.encode(input_text)\n",
    "print(\"Tokens after adding FY:\", len(tokens_after.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62726955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7effa40659d24e8896a5ce32cbc816b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a111addd37f47bbbd79ae2c351eb985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()enizer_config.json\";:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee65587daf684944875ef5fb7dbbbaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"config.json\";:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faacb76ee7f41bba1d7f94792683f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2846cebaf2245c7b4322b1f22bc488f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0596d6c85614434874151fe645dc35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()enizer_config.json\";:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b4cd7b87c443b28c60df9179656c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"config.json\";:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f668a655fc341469fa1dbd0d088ebf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"vocab.json\";:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f2670e641c4157a47bd517f07f8aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"merges.txt\";:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d0c6d40b0499db69d81f49fd246e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"tokenizer.json\";:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "GPT2 special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"BERT special tokens:\", bert_tok.special_tokens_map)\n",
    "print(\"GPT2 special tokens:\", gpt2_tok.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee1a1b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b84fe049244849bf260abf9e6b38e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/plain_text to /home/sachin/.cache/huggingface/datasets/parquet/plain_text-c403a23b02a09219/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1795134e9572439a8090ffb9c5f74f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0700db1830e4e8b95e2a50e4f93fe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b6a3e99c524a068ee6137a0ec64b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36abe3e39014fd682bdd90cb8e81948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ExpectedMoreSplits",
     "evalue": "{'unsupervised'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExpectedMoreSplits\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m imdb \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m imdb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m imdb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define all tokenizers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/load.py:1809\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1820\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1821\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/builder.py:909\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/builder.py:1022\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     dl_manager\u001b[38;5;241m.\u001b[39mmanage_extracted_files()\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS:\n\u001b[0;32m-> 1022\u001b[0m     \u001b[43mverify_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;241m=\u001b[39m split_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/utils/info_utils.py:91\u001b[0m, in \u001b[0;36mverify_splits\u001b[0;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(expected_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(recorded_splits)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExpectedMoreSplits(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mset\u001b[39m(expected_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(recorded_splits)))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(recorded_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(expected_splits)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedSplits(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mset\u001b[39m(recorded_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(expected_splits)))\n",
      "\u001b[0;31mExpectedMoreSplits\u001b[0m: {'unsupervised'}"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "all_texts = imdb[\"train\"][\"text\"] + imdb[\"test\"][\"text\"]\n",
    "\n",
    "# Define all tokenizers\n",
    "tokenizers_list = {\n",
    "    \"1\": tokenizer,  # Custom 32K tokenizer\n",
    "    \"2\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"3\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"4\": Tokenizer.from_file(\"hopper.json\")\n",
    "}\n",
    "\n",
    "# Count tokens\n",
    "token_counts = {}\n",
    "for k, tok in tokenizers_list.items():\n",
    "    total = 0\n",
    "    for text in all_texts:\n",
    "        if isinstance(tok, Tokenizer):\n",
    "            total += len(tok.encode(text).tokens)\n",
    "        else:\n",
    "            total += len(tok.encode(text).input_ids)\n",
    "    token_counts[k] = total\n",
    "\n",
    "print(\"Token counts:\", sorted(token_counts.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ff86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([8, 17])\n"
     ]
    }
   ],
   "source": [
    "batch = [\"This is a short sentence.\", \"This is a much longer sentence with more tokens than the previous one.\"] * 4\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\", padding=True, truncation=True, max_length=128)\n",
    "output = tok(batch, padding=True, return_tensors=\"pt\")\n",
    "print(\"Shape:\", output['input_ids'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
