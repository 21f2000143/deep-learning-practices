{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c290f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Layout, interact, interactive, fixed, interact_manual, widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b261062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29279808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "  pairs = collections.defaultdict(int)\n",
    "  for word, freq in vocab.items():\n",
    "    symbols = word.split()\n",
    "    for i in range(len(symbols)-1):\n",
    "      pairs[symbols[i],symbols[i+1]] += freq\n",
    "  return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "  v_out = {}\n",
    "  bigram = re.escape(' '.join(pair))\n",
    "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "  for word in v_in:\n",
    "  \tw_out = p.sub(''.join(pair), word)\n",
    "  \tv_out[w_out] = v_in[word]\n",
    "  return v_out\n",
    "\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n",
    "'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "  pairs = get_stats(vocab)\n",
    "  best = max(pairs, key=pairs.get)\n",
    "  vocab = merge_vocab(best, vocab)\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37aaa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display a pair of subtokens to be merged in a slider\n",
    "def get_pairs(pair:int):\n",
    "    \"\"\"\n",
    "    pair: index of the pair. \n",
    "    \"\"\"\n",
    "    if pair>0:\n",
    "        left, right = lines[pair].strip('\\n').split(' ')\n",
    "        print(f'{left} , {right}')\n",
    "        \n",
    "# to display token ids  in a slider\n",
    "def display_token_id(id):\n",
    "    token,id = vocab_sorted[id]\n",
    "    print(f'id:{id} \\t token:{token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5a81d",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ac0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e52c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (/home/sachin/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 74004228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "subset = load_dataset('bookcorpus',split='all')\n",
    "pprint(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53caa55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset.select(range(0, len(subset), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f3c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10572033\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2776f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['usually , he would be tearing around the living room , playing with his toys .',\n",
       "  'mason barely acknowledged her .',\n",
       "  'mason was already registering off the charts in height and weight according to his pediatrician .',\n",
       "  'she never wanted anything in the world to hurt him , and she knew that being rejected by his father would .',\n",
       "  \"aidan was her mother 's baby brother and only son of the family .\",\n",
       "  \"while it had been no question that she wanted him as godfather for mason , she had been extremely honored when he and his wife , emma , had asked her to be their son , noah 's , godmother .\"]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "732f491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1da8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPE(unk_token = '[UNK]')\n",
    "tokenizer = Tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b626a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2466b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "def trainer_with_vocab_size(vocab_size=10000):\n",
    "  trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=['GO', 'UNK', 'PAD', 'EOS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b6b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(batch_size=1000):\n",
    "  for i in range(0, len(subset), batch_size):\n",
    "    yield subset[i: i+batch_size]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6802079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer with the vocab_size \n",
    "trainer = trainer_with_vocab_size(vocab_size=5000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer, length=len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff618fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('hopper5k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aafb0d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce601c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer10 = trainer_with_vocab_size(vocab_size=10000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer10, length=len(subset))\n",
    "tokenizer.save('hopper10k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03d735c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer15 = trainer_with_vocab_size(vocab_size=15000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer15, length=len(subset))\n",
    "tokenizer.save('hopper15k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d7132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer32 = trainer_with_vocab_size(vocab_size=32000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer32, length=len(subset))\n",
    "tokenizer.save('hopper32k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "524ee28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())\n",
    "trained_tokenizer = trained_tokenizer.from_file('hopper5k.json')\n",
    "tokens = trained_tokenizer.encode(input_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aeae3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())\n",
    "trained_tokenizer = trained_tokenizer.from_file('hopper10k.json')\n",
    "tokens = trained_tokenizer.encode(input_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db6a97ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=25, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())\n",
    "trained_tokenizer = trained_tokenizer.from_file('hopper.json')\n",
    "tokens = trained_tokenizer.encode(input_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a513db4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.models.BPE at 0x7df0cb11b070>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.from_file('./model/hopper10-vocab.json', 'model/hopper10-merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57eed68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80fc6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.models.BPE' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model/hopper15-vocab.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/hopper15-merges.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEBI study finds 93\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mf individual F&O traders made losses between FY22 and FY24.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(input_text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.models.BPE' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "trainer10 = trainer_with_vocab_size(vocab_size=15000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer10, length=len(subset))\n",
    "tokenizer.model.save('model', prefix='hopper15')\n",
    "tokenizer = tokenizer.model.from_file('./model/hopper15-vocab.json', 'model/hopper15-merges.txt')\n",
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617587fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Token count: Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "trainer10 = trainer_with_vocab_size(vocab_size=32000)\n",
    "tokenizer.train_from_iterator(get_examples(batch_size=10000), trainer=trainer10, length=len(subset))\n",
    "tokenizer.model.save('model', prefix='hopper32')\n",
    "tokenizer = tokenizer.model.from_file('./model/hopper32-vocab.json', 'model/hopper32-merges.txt')\n",
    "input_text = \"SEBI study finds 93% of individual F&O traders made losses between FY22 and FY24.\"\n",
    "output = tokenizer.encode(input_text)\n",
    "print(\"Token count:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff48c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=5000,  # Change to 10000, 15000, 32000 as needed\n",
    "    special_tokens=[\"[GO]\", \"[UNK]\", \"[PAD]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "# Suppose `dataset` is a list of 10,572,033 strings\n",
    "# dataset = load_bookcorpus_every_7th_sample()\n",
    "tokenizer.train_from_iterator(subset, trainer)\n",
    "\n",
    "# Save and reload for reuse\n",
    "tokenizer.save(\"custom_bpe_5000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b2ce758",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m----> 3\u001b[0m hopper_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhopper.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m hopper_tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens (hopper):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens\u001b[38;5;241m.\u001b[39mtokens))\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "hopper_tokenizer = Tokenizer.from_file(\"hopper.json\")\n",
    "tokens = hopper_tokenizer.encode(input_text)\n",
    "print(\"Tokens (hopper):\", len(tokens.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75351b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hopper_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhopper_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39madd_tokens([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFY\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m tokens_after \u001b[38;5;241m=\u001b[39m hopper_tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens after adding FY:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens_after\u001b[38;5;241m.\u001b[39mtokens))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hopper_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "hopper_tokenizer.model.add_tokens([\"FY\"])\n",
    "tokens_after = hopper_tokenizer.encode(input_text)\n",
    "print(\"Tokens after adding FY:\", len(tokens_after.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62726955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7effa40659d24e8896a5ce32cbc816b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a111addd37f47bbbd79ae2c351eb985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enizer_config.json\";:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee65587daf684944875ef5fb7dbbbaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"config.json\";:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faacb76ee7f41bba1d7f94792683f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2846cebaf2245c7b4322b1f22bc488f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0596d6c85614434874151fe645dc35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enizer_config.json\";:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b4cd7b87c443b28c60df9179656c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"config.json\";:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f668a655fc341469fa1dbd0d088ebf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"vocab.json\";:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f2670e641c4157a47bd517f07f8aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"merges.txt\";:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d0c6d40b0499db69d81f49fd246e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "GPT2 special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"BERT special tokens:\", bert_tok.special_tokens_map)\n",
    "print(\"GPT2 special tokens:\", gpt2_tok.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee1a1b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b84fe049244849bf260abf9e6b38e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/plain_text to /home/sachin/.cache/huggingface/datasets/parquet/plain_text-c403a23b02a09219/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1795134e9572439a8090ffb9c5f74f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0700db1830e4e8b95e2a50e4f93fe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b6a3e99c524a068ee6137a0ec64b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36abe3e39014fd682bdd90cb8e81948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ExpectedMoreSplits",
     "evalue": "{'unsupervised'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExpectedMoreSplits\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m imdb \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m imdb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m imdb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define all tokenizers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/load.py:1809\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1820\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1821\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/builder.py:909\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/builder.py:1022\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     dl_manager\u001b[38;5;241m.\u001b[39mmanage_extracted_files()\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS:\n\u001b[0;32m-> 1022\u001b[0m     \u001b[43mverify_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;241m=\u001b[39m split_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/lmlf/lib/python3.10/site-packages/datasets/utils/info_utils.py:91\u001b[0m, in \u001b[0;36mverify_splits\u001b[0;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(expected_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(recorded_splits)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExpectedMoreSplits(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mset\u001b[39m(expected_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(recorded_splits)))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(recorded_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(expected_splits)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedSplits(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mset\u001b[39m(recorded_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(expected_splits)))\n",
      "\u001b[0;31mExpectedMoreSplits\u001b[0m: {'unsupervised'}"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "all_texts = imdb[\"train\"][\"text\"] + imdb[\"test\"][\"text\"]\n",
    "\n",
    "# Define all tokenizers\n",
    "tokenizers_list = {\n",
    "    \"1\": tokenizer,  # Custom 32K tokenizer\n",
    "    \"2\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"3\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"4\": Tokenizer.from_file(\"hopper.json\")\n",
    "}\n",
    "\n",
    "# Count tokens\n",
    "token_counts = {}\n",
    "for k, tok in tokenizers_list.items():\n",
    "    total = 0\n",
    "    for text in all_texts:\n",
    "        if isinstance(tok, Tokenizer):\n",
    "            total += len(tok.encode(text).tokens)\n",
    "        else:\n",
    "            total += len(tok.encode(text).input_ids)\n",
    "    token_counts[k] = total\n",
    "\n",
    "print(\"Token counts:\", sorted(token_counts.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ff86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([8, 17])\n"
     ]
    }
   ],
   "source": [
    "batch = [\"This is a short sentence.\", \"This is a much longer sentence with more tokens than the previous one.\"] * 4\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\", padding=True, truncation=True, max_length=128)\n",
    "output = tok(batch, padding=True, return_tensors=\"pt\")\n",
    "print(\"Shape:\", output['input_ids'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmlf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
