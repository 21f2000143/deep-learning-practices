{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3c3c73",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047641d8",
   "metadata": {},
   "source": [
    "## Import the necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778a4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in the previous experiments\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# for training\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# for experiment tracking\n",
    "import wandb\n",
    "\n",
    "\n",
    "# common packages\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48559160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_full = load_dataset('bookcorpus', split='all')\n",
    "# pprint(ds_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e2d41",
   "metadata": {},
   "source": [
    "## Distribution of the length of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b59ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the length of each sample (number of words per sample)\n",
    "# sample_lengths = [len(text.split()) for text in ds_full['text']]\n",
    "# sample_lengths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8aac64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.unique(sample_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the distribution\n",
    "# import random\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.hist(random.sample(sample_lengths, k=10**6), bins=bins[0:150])\n",
    "# plt.xlabel('Sequence length')\n",
    "# plt.ylabel('Count')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03243180",
   "metadata": {},
   "source": [
    "## Tokenization from the previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b10ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hopper_tokenizer = AutoTokenizer.from_pretrained('../week-2/hopper')\n",
    "# # hopper_tokenizer\n",
    "# auto_loaded_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"../week-2/awesome_tokenizer\", \n",
    "#     local_files_only=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13869c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hopper_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"../week-2/awesome_tokenizer\", \n",
    "#     local_files_only=True\n",
    "# )\n",
    "# hopper_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8340a",
   "metadata": {},
   "source": [
    "## Apply for batch samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23919e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs =4 # batch_size\n",
    "# model_inputs = hopper_tokenizer(ds_full[0:bs]['text'], padding=True)\n",
    "# pprint(model_inputs['input_ids'], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3cbdb9",
   "metadata": {},
   "source": [
    "## Instead of the hopper tokenizer, use gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed382e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea938d",
   "metadata": {},
   "source": [
    "> The parameter `padding_side` is set to `right`, but since no padding token is used, we should add a padding token to avoid errors from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a84ca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdab52b",
   "metadata": {},
   "source": [
    "## Map Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e55534",
   "metadata": {},
   "source": [
    "### Custom mapping function\n",
    "\n",
    "> Which takes a batch of samples and returns `input_ids` and `attention_mask` such that the length of `input_ids` is 1024 for all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8c9c1",
   "metadata": {},
   "source": [
    "### Apply Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a7199",
   "metadata": {},
   "source": [
    "> Define a mapping function that takes a batch of samplews and returns input_ids and attention_mask such that the length of input_ids is 1024 for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1411f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "008e342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cust_func(batch_size):\n",
    "#   return tokenizer(\n",
    "#     batch_size['text'],\n",
    "#     padding = 'max_length',\n",
    "#     truncation = True,\n",
    "#     max_length = 1024,\n",
    "#     return_attention_mask=True\n",
    "#       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f8388",
   "metadata": {},
   "source": [
    "## Apply the mapping to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c8bd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_ds = ds_full.map(\n",
    "#   cust_func,\n",
    "#   batched=True,\n",
    "#   remove_columns=ds_full.column_names # removes original text\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "082cc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_ds.save_to_disk('data/BC_Chunked')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c380c",
   "metadata": {},
   "source": [
    "## Access the save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a380c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_chunked = load_from_disk('data/BC_Chunked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c1496c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 74004228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57cba0e",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "422e66a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/sachin/projects/DLP/deep-learning-practices/week-3/data/BC_Chunked/cache-972c6039c69922a1.arrow and /home/sachin/projects/DLP/deep-learning-practices/week-3/data/BC_Chunked/cache-71041fb06f640bba.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 73486198\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 518030\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_split = ds_chunked.train_test_split(test_size=0.007, seed=42)\n",
    "ds_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82dcca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "985f8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2662f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=ds_split['train'],\n",
    "                        collate_fn=data_collator,\n",
    "                        batch_size=4,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a95b133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]),\n",
      " 'input_ids': tensor([[15506,  2513,   494,  ..., 50256, 50256, 50256],\n",
      "        [36151,   764, 50256,  ..., 50256, 50256, 50256],\n",
      "        [ 1820, 22662,   275,  ..., 50256, 50256, 50256],\n",
      "        [   71,  3020,   837,  ..., 50256, 50256, 50256]]),\n",
      " 'labels': tensor([[15506,  2513,   494,  ...,  -100,  -100,  -100],\n",
      "        [36151,   764,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ 1820, 22662,   275,  ...,  -100,  -100,  -100],\n",
      "        [   71,  3020,   837,  ...,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "  pprint(batch)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca226708",
   "metadata": {},
   "source": [
    "## Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0061ba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the configuration from the HF hub\n",
    "configuration = GPT2Config()\n",
    "pprint(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc5812",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33c0b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(configuration)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019283af",
   "metadata": {},
   "source": [
    "## Count the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff5ddf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 124.44M\n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "for param in model.parameters():\n",
    "  num_parameters +=param.numel()\n",
    "print(f'Number of Parameters: {num_parameters/10**6:.2f}M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd37b76",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983a11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "969501f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be049081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m21f2000143\u001b[0m (\u001b[33m21f2000143-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sachin/projects/DLP/deep-learning-practices/week-3/wandb/run-20250627_090215-s4e7dwhj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/21f2000143-indian-institute-of-technology-madras/my-awesome-project/runs/s4e7dwhj' target=\"_blank\">gallant-disco-3</a></strong> to <a href='https://wandb.ai/21f2000143-indian-institute-of-technology-madras/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/21f2000143-indian-institute-of-technology-madras/my-awesome-project' target=\"_blank\">https://wandb.ai/21f2000143-indian-institute-of-technology-madras/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/21f2000143-indian-institute-of-technology-madras/my-awesome-project/runs/s4e7dwhj' target=\"_blank\">https://wandb.ai/21f2000143-indian-institute-of-technology-madras/my-awesome-project/runs/s4e7dwhj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/21f2000143-indian-institute-of-technology-madras/my-awesome-project/runs/s4e7dwhj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x77366ab55ba0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb for experiment tracking\n",
    "wandb.init(\n",
    "  project='my-awesome-project',\n",
    "  config={\n",
    "    \"batch_size\":16,\n",
    "    \"dataset\":\"Bookcorpus-74M\",\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1138dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  overwrite_output_dir=True,\n",
    "  num_train_epochs=1,\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=500,\n",
    "  save_steps=500,\n",
    "  logging_steps=100,\n",
    "  save_total_limit=2,\n",
    "  learning_rate=5e-5,\n",
    "  weight_decay=0.01,\n",
    "  fp16=True,\n",
    "  report_to=\"wandb\",\n",
    "  run_name=\"gpt2-bookcorpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98c461a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=ds_split['train'],\n",
    "                  eval_dataset = ds_split['test'],\n",
    "                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4154eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/miniconda3/envs/lmlf/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73486198\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18371550\n",
      "  Number of trainable parameters = 124439808\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "  Num examples = 73486198\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18371550\n",
      "  Number of trainable parameters = 124439808\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmlf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
